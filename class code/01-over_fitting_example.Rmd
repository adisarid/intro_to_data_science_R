---
title: "Overfitting illustrated in Linear Regression"
author: "Adi Sarid"
date: "July 2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(0)
```

When modelling, we usually split the data to train/test sets.
We train the model on the train set, and test it on the test set.

When we take to many features, compared to the sample size $n$, we risk the curse of dimensionality, also reffered to as **overfitting** the model. 

The ramifications of overfitting can be illustrated using a linear model. We will later on discuss how **variable selection** and **dimension reduction** can be used to address this issue.

## Illustration of overfitting

   * Take $y$ completely random with 100 observations.
   * Take 95 parameters $x_1,\ldots,x_{95}$ and set their values randomly also.
   * Build a model, any model (we'll use linear regression here).
   * Analyze the model's fit.
   * Do the process only this time with a train/test split.
   
```{r overfitting in action, message=FALSE, warning=FALSE}
library(tidyverse)
xvars <- data.frame(matrix(runif(100*95), ncol=95))
overfitting <- tibble(y = runif(100)) %>%
  bind_cols(xvars)
glimpse(overfitting)
ggplot(overfitting, aes(y)) + geom_histogram()
# these are just uniformly distributed numbers, should have no kind of relationship between variables
# here's a model with just a few X's, and no overfit. The model is insignificant.
# the only significant coefficient beta is the intercept (which is roughly equal to the average of y)
lm_no_overfit <- lm(data = overfitting,
                    formula = y ~ X1 + X2 + X3)
summary(lm_no_overfit)

# now, see what happens when we add all the 95 features
# mostly, look at the R^2. It's almost 1!
lm_overfit <- lm(data = overfitting,
                 formula = y ~ .)
summary(lm_overfit)
# now, see the errors of each model
overfitting <- overfitting %>% 
  mutate(res_no_overfit = y - predict(lm_no_overfit, newdata = overfitting),
         res_overfit = y - predict(lm_overfit, newdata = overfitting))
overfitting %>%
  summarize(error_overfit = mean(abs(res_overfit)),
            error_no_overfit = mean(abs(res_no_overfit)))
```

It looks as if the new model's fit is very good, using 95 variables reduced the error significantly. 

Also note the the $R^2=0.98$ and the Adjusted $R=0.62$. Obviously, this is a bluf.

Let's do this again, only this time with a train/test split.

```{r overfitting detection with test set}
overfitting <- overfitting %>%
  mutate(is_train = runif(nrow(overfitting)) < 0.8)

lm_overfit_train <- lm(data = overfitting %>% filter(is_train),
                       formula = y ~ .)
lm_no_overfit_train <- lm(data = overfitting %>% filter(is_train),
                       formula = y ~ X1 +X2 +X3)

overfitting <- overfitting %>%
  mutate(res_overfit = y - predict(lm_overfit_train, newdata = overfitting),
         res_no_overfit = y - predict(lm_no_overfit_train, newdata = overfitting))

overfitting %>%
  group_by(is_train) %>% 
  summarize(error_overfit = mean(abs(res_overfit)),
            error_nooverfit = mean(abs(res_no_overfit)))

# Now the "true face" of the model is discovered. See how high the error rate of the test set is!
# Beware of overfitting models. Always use train/test. Watch out for n and p.
```

## To sum up

   * Beware of overfitting.
   * Always use a train/test split (also possible train/test/validate or cross-validation).
   * Consider the number of parameters $p$ versus the sample size $n$. There is no "iron rule" here but the test set error will help guide you, and also, comparing a nominal model to your model will show you the contribution of your model.
   * We will now discuss additional techniques to help against overfitting by reducing the number of features:
      * Feature selection
      * Dimension reduction
      * Feature importance
      
